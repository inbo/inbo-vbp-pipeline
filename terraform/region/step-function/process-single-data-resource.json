{
  "Comment": "Starts an EMR cluster, processes data resources in parallel, and terminates the cluster",
  "StartAt": "Get State-machine Config",
  "States": {
    "Get State-machine Config": {
      "Type": "Task",
      "Arguments": {
        "Bucket": "inbo-vbp-dev-pipelines",
        "Key": "config/state-machine.json"
      },
      "Resource": "arn:aws:states:::aws-sdk:s3:getObject",
      "Next": "Get Data Resource Details",
      "Assign": {
        "dataResourceId": "{% $states.input.dataResourceId %}",
        "config": "{% $parse($states.result.Body) %}",
        "rootPipelineId": "{% $exists($states.context.Execution.Input.rootPipelineId) ? $states.context.Execution.Input.rootPipelineId : $states.context.Execution.Id %}"
      }
    },
    "Get Data Resource Details": {
      "Type": "Task",
      "Resource": "arn:aws:states:::http:invoke",
      "Arguments": {
        "ApiEndpoint": "{% 'https://' & $config.base_domain & '/collectory/ws/dataResource/' &$dataResourceId %}",
        "Method": "GET",
        "Authentication": {
          "ConnectionArn": "{% $config.collectory_authenticated_connection_arn %}"
        }
      },
      "Retry": [
        {
          "ErrorEquals": [
            "States.ALL"
          ],
          "BackoffRate": 2,
          "IntervalSeconds": 1,
          "MaxAttempts": 3,
          "JitterStrategy": "FULL"
        }
      ],
      "Next": "Has a download URL",
      "Assign": {
        "dataResourceId": "{% $states.result.ResponseBody.uid %}",
        "lastUpdated": "{% $states.result.ResponseBody.lastUpdated %}"
      }
    },
    "Has a download URL": {
      "Type": "Choice",
      "Choices": [
        {
          "Condition": "{% $exists($states.input.ResponseBody.connectionParameters.url) %}",
          "Next": "Lock Data Resource for Processing",
          "Assign": {
            "dataResourceUrl": "{% $states.input.ResponseBody.connectionParameters.url %}"
          }
        }
      ],
      "Default": "Store Execution State Failed",
      "Output": {
        "Error": "MissingConnectionParams",
        "Cause": "The Data Resources does not have any connection params configured"
      }
    },
    "Store Execution State Failed": {
      "Type": "Task",
      "Resource": "arn:aws:states:::dynamodb:putItem",
      "Arguments": {
        "TableName": "{% $config.dynamodb_table_name %}",
        "Item": {
          "PK": {
            "S": "{% $rootPipelineId %}"
          },
          "SK": {
            "S": "{% 'RUN#' & $dataResourceId %}"
          },
          "State": {
            "S": "FAILED"
          },
          "Error": {
            "S": "{% $states.input.Error %}"
          },
          "Cause": {
            "S": "{% $states.input.Cause %}"
          }
        }
      },
      "Next": "Fail",
      "Output": "{% $states.input %}"
    },
    "Lock Data Resource for Processing": {
      "Type": "Task",
      "Arguments": {
        "TransactItems": [
          {
            "Put": {
              "TableName": "{% $config.dynamodb_table_name %}",
              "Item": {
                "PK": {
                  "S": "{% 'DATA_RESOURCE#' & $dataResourceId %}"
                },
                "SK": {
                  "S": "LOCK"
                },
                "LockedBy": {
                  "S": "{% $states.context.Execution.Id %}"
                },
                "TTL": {
                  "N": "{% $string($millis() + 86400000) %}"
                }
              },
              "ConditionExpression": "{% $exists($states.context.Execution.Input.overwriteLock) and $states.context.Execution.Input.overwriteLock ? null : 'attribute_not_exists(SK)' %}"
            }
          }
        ]
      },
      "Resource": "arn:aws:states:::aws-sdk:dynamodb:transactWriteItems",
      "Next": "Catch Failures",
      "Catch": [
        {
          "ErrorEquals": [
            "States.ALL"
          ],
          "Next": "Store Execution State Failed"
        }
      ]
    },
    "Catch Failures": {
      "Type": "Parallel",
      "Next": "Unlock Data Resource for Processing",
      "Branches": [
        {
          "StartAt": "Should Cleanup Old Pipeline Data",
          "States": {
            "Should Cleanup Old Pipeline Data": {
              "Type": "Choice",
              "Choices": [
                {
                  "Next": "Cleanup Old Pipeline Data",
                  "Condition": "{% $exists($states.context.Execution.Input.shouldCleanupOldPipelineData) and $states.context.Execution.Input.shouldCleanupOldPipelineData %}"
                },
                {
                  "Next": "Download Data Resource",
                  "Condition": "{% $exists($states.context.Execution.Input.shouldRedownload) and $states.context.Execution.Input.shouldRedownload %}"
                }
              ],
              "Default": "Get Processing History",
              "Output": "{% $lastUpdated %}"
            },
            "Get Processing History": {
              "Type": "Task",
              "Arguments": {
                "TableName": "{% $config.dynamodb_table_name %}",
                "KeyConditionExpression": "PK = :pk and begins_with(SK, :sk)",
                "ExpressionAttributeValues": {
                  ":pk": {
                    "S": "{% 'DATA_RESOURCE#' & $dataResourceId %}"
                  },
                  ":sk": {
                    "S": "{% 'HISTORY#' & $lastUpdated %}"
                  }
                },
                "ScanIndexForward": false,
                "Limit": 1
              },
              "Resource": "arn:aws:states:::aws-sdk:dynamodb:query",
              "Next": "Can Skip Processing?",
              "Output": "{% $states.result.Count = 1 ? $states.result.Items[0] : {} %}"
            },
            "Cleanup Old Pipeline Data": {
              "Type": "Task",
              "Resource": "arn:aws:states:::batch:submitJob.sync",
              "Arguments": {
                "JobDefinition": "{% $config.delete_data_resource_data_job_definition_arn %}",
                "JobName": "{% 'delete-data-' & $dataResourceId %}",
                "JobQueue": "{% $config.job_queue_arn %}",
                "ContainerOverrides": {
                  "Environment": [
                    {
                      "Name": "DATA_RESOURCE_ID",
                      "Value": "{% $dataResourceId %}"
                    }
                  ]
                }
              },
              "Next": "Download Data Resource"
            },
            "Can Skip Processing?": {
              "Type": "Choice",
              "Choices": [
                {
                  "Next": "Do Nothing",
                  "Condition": "{% $exists($states.input.Event.S) and $states.input.Event.S = 'SOLR_UPLOADED' and $states.context.Execution.Input.solrCollection = $states.input.SolrCollections.S %}"
                },
                {
                  "Next": "Get or create EMR Cluster 2",
                  "Condition": "{% $exists($states.input.Event.S) and $states.input.Event.S = 'SAMPLED'  %}"
                },
                {
                  "Next": "Get or create EMR Cluster",
                  "Condition": "{% $exists($states.input.Event.S) and $states.input.Event.S = 'INDEXED'  %}"
                },
                {
                  "Next": "Is Big or Small?",
                  "Condition": "{% $exists($states.input.Event.S) and $states.input.Event.S = 'DOWNLOADED' %}",
                  "Output": "{% $number( $states.input.FileSize.N) %}"
                }
              ],
              "Default": "Download Data Resource",
              "Output": "{% $states.input %}"
            },
            "Get or create EMR Cluster 2": {
              "Type": "Task",
              "Resource": "arn:aws:states:::states:startExecution.sync:2",
              "Arguments": {
                "StateMachineArn": "{% $config.get_or_create_emr_cluster_state_machine_arn %}",
                "Input": {
                  "AWS_STEP_FUNCTIONS_STARTED_BY_EXECUTION_ID": "{% $states.context.Execution.Id %}",
                  "rootPipelineId": "{% $rootPipelineId %}"
                }
              },
              "Assign": {
                "clusterId": "{% $states.result.Output.clusterId %}"
              },
              "Next": "Parallel",
              "Output": {
                "skipSamplingSync": true
              }
            },
            "Download Data Resource": {
              "Type": "Task",
              "Resource": "arn:aws:states:::batch:submitJob.sync",
              "Arguments": {
                "JobDefinition": "{% $config.download_data_resource_job_definition_arn %}",
                "JobName": "{% 'download-data-' & $dataResourceId %}",
                "JobQueue": "{% $config.job_queue_arn %}",
                "ContainerOverrides": {
                  "Environment": [
                    {
                      "Name": "DATA_RESOURCE_ID",
                      "Value": "{% $dataResourceId %}"
                    },
                    {
                      "Name": "DATA_RESOURCE_URL",
                      "Value": "{% $dataResourceUrl %}"
                    },
                    {
                      "Name": "DATA_RESOURCE_LAST_UPDATED",
                      "Value": "{% $lastUpdated %}"
                    },
                    {
                      "Name": "DOWNLOADED_EVENT_TIMESTAMP",
                      "Value": "{% $now() %}"
                    },
                    {
                      "Name": "ROOT_PIPELINE_ID",
                      "Value": "{% $rootPipelineId %}"
                    },
                    {
                      "Name": "EXECUTION_PIPELINE_ID",
                      "Value": "{% $states.context.Execution.Id %}"
                    }
                  ]
                }
              },
              "Next": "Get Download File Size and Hash",
              "Output": {
                "downloadedTimeStamp": "{% $states.result.Container.Environment[Name='DOWNLOADED_EVENT_TIMESTAMP'].Value %}"
              }
            },
            "Get Download File Size and Hash": {
              "Type": "Task",
              "Resource": "arn:aws:states:::dynamodb:getItem",
              "Arguments": {
                "TableName": "{% $config.dynamodb_table_name %}",
                "Key": {
                  "PK": {
                    "S": "{% 'DATA_RESOURCE#' & $dataResourceId %}"
                  },
                  "SK": {
                    "S": "{% $join(['HISTORY', $lastUpdated, $states.input.downloadedTimeStamp, 'DOWNLOADED'], '#') %}"
                  }
                }
              },
              "Next": "Store File Hash Reference",
              "Assign": {
                "fileSize": "{% $number($states.result.Item.FileSize.N) %}",
                "fileHash": "{% $states.result.Item.FileHash.S %}"
              }
            },
            "Store File Hash Reference": {
              "Type": "Task",
              "Resource": "arn:aws:states:::dynamodb:putItem",
              "Arguments": {
                "TableName": "{% $config.dynamodb_table_name %}",
                "Item": {
                  "PK": {
                    "S": "{% 'DATA_RESOURCE#' & $dataResourceId %}"
                  },
                  "SK": {
                    "S": "{% 'FILE_HASH#' & $fileHash %}"
                  },
                  "FileHash": {
                    "S": "{% $fileHash %}"
                  },
                  "LastUpdated": {
                    "S": "{% $lastUpdated %}"
                  }
                },
                "ReturnValues": "ALL_OLD"
              },
              "Next": "Still has the same File Hash?",
              "Output": "{% $exists($states.result.Attributes) ? $states.result.Attributes : {} %}"
            },
            "Still has the same File Hash?": {
              "Type": "Choice",
              "Choices": [
                {
                  "Next": "Get Processing History",
                  "Condition": "{% $not($exists($states.context.Execution.Input.shouldCleanupOldPipelineData) and $states.context.Execution.Input.shouldCleanupOldPipelineData) and $exists($states.input.LastUpdated.S) %}",
                  "Output": "{% $states.input.LastUpdated.S %}"
                }
              ],
              "Default": "Is Big or Small?",
              "Output": "{% $fileSize %}"
            },
            "Is Big or Small?": {
              "Type": "Choice",
              "Choices": [
                {
                  "Next": "Process Large Dataresource",
                  "Condition": "{% $states.input > $config.dataresource_size_threshold %}"
                }
              ],
              "Default": "Process Small Data Resource"
            },
            "Process Large Dataresource": {
              "Type": "Task",
              "Resource": "arn:aws:states:::states:startExecution.sync:2",
              "Arguments": {
                "StateMachineArn": "{% $config.process_large_dataresource_state_machine_arn %}",
                "Input": {
                  "AWS_STEP_FUNCTIONS_STARTED_BY_EXECUTION_ID": "{% $states.context.Execution.Id %}",
                  "dataResourceId": "{% $dataResourceId %}",
                  "rootPipelineId": "{% $rootPipelineId %}"
                }
              },
              "Next": "Update Index Success"
            },
            "Process Small Data Resource": {
              "Type": "Task",
              "Resource": "arn:aws:states:::batch:submitJob.sync",
              "Arguments": {
                "JobDefinition": "{% $config.process_data_resource_job_definition_arn %}",
                "JobName": "{% 'process-data-resource-' & $dataResourceId %}",
                "JobQueue": "{% $config.job_queue_arn %}",
                "ContainerOverrides": {
                  "Environment": [
                    {
                      "Name": "DATA_RESOURCE_ID",
                      "Value": "{% $dataResourceId %}"
                    }
                  ]
                }
              },
              "Next": "Update Index Success"
            },
            "Update Index Success": {
              "Type": "Task",
              "Resource": "arn:aws:states:::dynamodb:putItem",
              "Arguments": {
                "TableName": "{% $config.dynamodb_table_name %}",
                "Item": {
                  "PK": {
                    "S": "{% 'DATA_RESOURCE#' & $dataResourceId %}"
                  },
                  "SK": {
                    "S": "{% $join(['HISTORY', $lastUpdated, $now(), 'INDEXED'], '#') %}"
                  },
                  "RootPipelineId": {
                    "S": "{% $rootPipelineId %}"
                  },
                  "ExecutionId": {
                    "S": "{% $states.context.Execution.Id %}"
                  },
                  "DataResourceId": {
                    "S": "{% $dataResourceId %}"
                  },
                  "Timestamp": {
                    "S": "{% $now() %}"
                  },
                  "Event": {
                    "S": "INDEXED"
                  }
                }
              },
              "Next": "Get or create EMR Cluster"
            },
            "Get or create EMR Cluster": {
              "Type": "Task",
              "Resource": "arn:aws:states:::states:startExecution.sync:2",
              "Arguments": {
                "StateMachineArn": "{% $config.get_or_create_emr_cluster_state_machine_arn %}",
                "Input": {
                  "AWS_STEP_FUNCTIONS_STARTED_BY_EXECUTION_ID": "{% $states.context.Execution.Id %}",
                  "rootPipelineId": "{% $rootPipelineId %}"
                }
              },
              "Assign": {
                "clusterId": "{% $states.result.Output.clusterId %}"
              },
              "Next": "Sync EFS Data to HDFS"
            },
            "Sync EFS Data to HDFS": {
              "Type": "Task",
              "Resource": "arn:aws:states:::elasticmapreduce:addStep.sync",
              "Arguments": {
                "ClusterId": "{% $clusterId %}",
                "Step": {
                  "Name": "{% 'sync-efs-index-data-to-hdfs-' & $dataResourceId %}",
                  "ActionOnFailure": "CONTINUE",
                  "HadoopJarStep": {
                    "Jar": "s3://eu-west-1.elasticmapreduce/libs/script-runner/script-runner.jar",
                    "Args": [
                      "file:/opt/inbo/pipelines/bootstrap-actions/sync-index-data-to-hdfs.sh",
                      "{% $states.context.Execution.Input.dataResourceId %}"
                    ]
                  }
                }
              },
              "Next": "Sampling",
              "Retry": [
                {
                  "ErrorEquals": [
                    "States.TaskFailed"
                  ],
                  "BackoffRate": 2,
                  "IntervalSeconds": 1,
                  "MaxAttempts": 3,
                  "Comment": "Fails sometimes waiting for file copy?"
                }
              ]
            },
            "Sampling": {
              "Type": "Task",
              "Resource": "arn:aws:states:::elasticmapreduce:addStep.sync",
              "Arguments": {
                "ClusterId": "{% $clusterId %}",
                "Step": {
                  "Name": "{% 'sampling-' & $dataResourceId %}",
                  "ActionOnFailure": "CONTINUE",
                  "HadoopJarStep": {
                    "Jar": "command-runner.jar",
                    "Args": [
                      "spark-submit",
                      "--deploy-mode",
                      "client",
                      "--class",
                      "au.org.ala.pipelines.beam.SamplingPipeline",
                      "{% '/opt/inbo/pipelines/pipelines-' & $config.pipelines_version & '.jar' %}",
                      "{% '--datasetId=' & $dataResourceId %}",
                      "--config=/opt/inbo/pipelines/config/la-pipelines.yaml"
                    ]
                  }
                }
              },
              "Next": "Sample layers"
            },
            "Sample layers": {
              "Type": "Task",
              "Resource": "arn:aws:states:::elasticmapreduce:addStep.sync",
              "Arguments": {
                "ClusterId": "{% $clusterId %}",
                "Step": {
                  "Name": "{% 'sample-layers-' & $dataResourceId %}",
                  "ActionOnFailure": "CONTINUE",
                  "HadoopJarStep": {
                    "Jar": "{% '/opt/inbo/pipelines/pipelines-' & $config.pipelines_version & '.jar' %}",
                    "MainClass": "au.org.ala.sampling.LayerCrawler",
                    "Args": [
                      "{% '--datasetId=' & $dataResourceId %}",
                      "--config=/opt/inbo/pipelines/config/la-pipelines.yaml",
                      "--inputPath=hdfs:///."
                    ]
                  }
                }
              },
              "Next": "Parallel",
              "Output": {
                "skipSamplingSync": false
              }
            },
            "Parallel": {
              "Type": "Parallel",
              "Branches": [
                {
                  "StartAt": "Can Skip Syncing Sampling Data",
                  "States": {
                    "Can Skip Syncing Sampling Data": {
                      "Type": "Choice",
                      "Choices": [
                        {
                          "Next": "Do Nothing 2",
                          "Condition": "{% $boolean($states.input.skipSamplingSync) %}"
                        }
                      ],
                      "Default": "Sync Indexed and Sampled EFS Data to HDFS"
                    },
                    "Do Nothing 2": {
                      "Type": "Pass",
                      "End": true
                    },
                    "Sync Indexed and Sampled EFS Data to HDFS": {
                      "Type": "Task",
                      "Resource": "arn:aws:states:::elasticmapreduce:addStep.sync",
                      "Arguments": {
                        "ClusterId": "{% $clusterId %}",
                        "Step": {
                          "Name": "{% 'sync-hdfs-index-data-to-efs-' & $dataResourceId %}",
                          "ActionOnFailure": "CONTINUE",
                          "HadoopJarStep": {
                            "Jar": "s3://eu-west-1.elasticmapreduce/libs/script-runner/script-runner.jar",
                            "Args": [
                              "file:/opt/inbo/pipelines/bootstrap-actions/sync-to-efs.sh",
                              "{% $states.context.Execution.Input.dataResourceId %}"
                            ]
                          }
                        }
                      },
                      "Next": "Update Sampling Success"
                    },
                    "Update Sampling Success": {
                      "Type": "Task",
                      "Resource": "arn:aws:states:::dynamodb:putItem",
                      "Arguments": {
                        "TableName": "{% $config.dynamodb_table_name %}",
                        "Item": {
                          "PK": {
                            "S": "{% 'DATA_RESOURCE#' & $dataResourceId %}"
                          },
                          "SK": {
                            "S": "{% $join(['HISTORY', $lastUpdated, $now(), 'SAMPLED'], '#') %}"
                          },
                          "RootPipelineId": {
                            "S": "{% $rootPipelineId %}"
                          },
                          "ExecutionId": {
                            "S": "{% $states.context.Execution.Id %}"
                          },
                          "DataResourceId": {
                            "S": "{% $dataResourceId %}"
                          },
                          "Timestamp": {
                            "S": "{% $now() %}"
                          },
                          "Event": {
                            "S": "SAMPLED"
                          }
                        }
                      },
                      "End": true
                    }
                  }
                },
                {
                  "StartAt": "Clear Old Records",
                  "States": {
                    "Clear Old Records": {
                      "Type": "Task",
                      "Resource": "arn:aws:states:::lambda:invoke",
                      "Output": "{% $states.result.Payload %}",
                      "Arguments": {
                        "FunctionName": "{% config.lambda_biocache_index_management_arn %}",
                        "Payload": {
                          "httpMethod": "POST",
                          "version": "2",
                          "headers": {
                            "content-type": "application/json"
                          },
                          "isBase64Encoded": false,
                          "rawQueryString": "",
                          "rawPath": "/",
                          "routeKey": "/",
                          "originatesFromStepFunction": true,
                          "body": "{% $string({\n  'query': 'mutation ClearDataResourceFromIndex($input: ClearDataResourceFromIndexInput!) { clearDataResourceFromIndex(input: $input) { indexId dataResourceId } }',\n'variables': {\n  'input': {\n    'indexId': $states.context.Execution.Input.solrCollection,\n  'dataResourceId': $dataResourceId\n  }\n}\n}) %}"
                        }
                      },
                      "Retry": [
                        {
                          "ErrorEquals": [
                            "Lambda.ServiceException",
                            "Lambda.AWSLambdaException",
                            "Lambda.SdkClientException",
                            "Lambda.TooManyRequestsException"
                          ],
                          "IntervalSeconds": 1,
                          "MaxAttempts": 3,
                          "BackoffRate": 2,
                          "JitterStrategy": "FULL"
                        }
                      ],
                      "Next": "Solr"
                    },
                    "Solr": {
                      "Type": "Task",
                      "Resource": "arn:aws:states:::elasticmapreduce:addStep.sync",
                      "Arguments": {
                        "ClusterId": "{% $clusterId %}",
                        "Step": {
                          "Name": "{% 'solr-' & $dataResourceId %}",
                          "ActionOnFailure": "CONTINUE",
                          "HadoopJarStep": {
                            "Jar": "command-runner.jar",
                            "Args": [
                              "spark-submit",
                              "--deploy-mode",
                              "client",
                              "--conf",
                              "spark.yarn.executor.memoryOverheadFactor=0.5",
                              "--class",
                              "au.org.ala.pipelines.beam.IndexRecordToSolrPipeline",
                              "{% '/opt/inbo/pipelines/pipelines-' & $config.pipelines_version & '.jar' %}",
                              "{% '--datasetId=' & $dataResourceId %}",
                              "--config=/opt/inbo/pipelines/config/la-pipelines.yaml",
                              "{% '--solrCollection=' & $states.context.Execution.Input.solrCollection %}"
                            ]
                          }
                        }
                      },
                      "Next": "Store Solr Upload Event"
                    },
                    "Store Solr Upload Event": {
                      "Type": "Task",
                      "Resource": "arn:aws:states:::dynamodb:putItem",
                      "Arguments": {
                        "TableName": "{% $config.dynamodb_table_name %}",
                        "Item": {
                          "PK": {
                            "S": "{% 'DATA_RESOURCE#' & $dataResourceId %}"
                          },
                          "SK": {
                            "S": "{% $join(['HISTORY', $lastUpdated, $now(), 'SOLR_UPLOADED'], '#') %}"
                          },
                          "RootPipelineId": {
                            "S": "{% $rootPipelineId %}"
                          },
                          "ExecutionId": {
                            "S": "{% $states.context.Execution.Id %}"
                          },
                          "DataResourceId": {
                            "S": "{% $dataResourceId %}"
                          },
                          "Timestamp": {
                            "S": "{% $now() %}"
                          },
                          "Event": {
                            "S": "SOLR_UPLOADED"
                          },
                          "SolrCollection": {
                            "S": "{% $states.context.Execution.Input.solrCollection %}"
                          }
                        }
                      },
                      "End": true
                    }
                  }
                }
              ],
              "End": true
            },
            "Do Nothing": {
              "Type": "Pass",
              "End": true
            }
          }
        }
      ],
      "Catch": [
        {
          "ErrorEquals": [
            "States.ALL"
          ],
          "Next": "Unlock Data Resource for Processing 2",
          "Output": "{% $states.errorOutput %}"
        }
      ]
    },
    "Unlock Data Resource for Processing 2": {
      "Type": "Task",
      "Resource": "arn:aws:states:::dynamodb:deleteItem",
      "Arguments": {
        "TableName": "{% $config.dynamodb_table_name %}",
        "Key": {
          "PK": {
            "S": "{% 'DATA_RESOURCE#' & $dataResourceId %}"
          },
          "SK": {
            "S": "LOCK"
          }
        }
      },
      "Next": "Store Execution State Failed",
      "Output": "{% $states.input %}"
    },
    "Unlock Data Resource for Processing": {
      "Type": "Task",
      "Resource": "arn:aws:states:::dynamodb:deleteItem",
      "Arguments": {
        "TableName": "{% $config.dynamodb_table_name %}",
        "Key": {
          "PK": {
            "S": "{% 'DATA_RESOURCE#' & $dataResourceId %}"
          },
          "SK": {
            "S": "LOCK"
          }
        }
      },
      "End": true
    },
    "Fail": {
      "Type": "Fail",
      "Error": "{% $states.input.Error %}",
      "Cause": "{% $states.input.Cause %}"
    }
  },
  "QueryLanguage": "JSONata"
}