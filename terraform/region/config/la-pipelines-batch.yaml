### run options:

run:
  # where to run: local, spark-embedded or spark-cluster
  platform: local
  local:
    # jar: we get the jar from our dev or production environment
    sparkTmp: /data/spark-tmp
    sparkMaster: ""
    dwcaTmp: /data/dwca-tmp
    dwcaImportDir: /data/biocache-load
  spark-embedded:
    dwcaTmp: /data/dwca-tmp
    dwcaImportDir: /data/biocache-load
    sparkTmp: /data/spark-tmp
    sparkMaster: ""
  spark-cluster:
    dwcaTmp: /data/dwca-tmp
    dwcaImportDir: /data/biocache-load
    jar: /usr/share/la-pipelines/la-pipelines.jar
    sparkSubmit: /data/spark/bin/spark-submit
    sparkTmp: /data/spark-tmp
    sparkMaster: spark://localhost:7077

# Common PipelineOptions
general:
  attempt: 0
  hdfsSiteConfig: ""
  coreSiteConfig: ""

fs:
  platform: local
  local:
    fsPath: /data
  hdfs:
    fsPath: hdfs://localhost:8020

copy:
  copyPath: "/data/dwca-export"
recordedByConfig:
  cacheSizeMb: 64
alaNameMatch:
  wsUrl: http://namematching-service.vbp.internal:9179
  timeoutSec: 30
  retryConfig:
    maxAttempts: 5
    initialIntervalMillis: 5000
sds:
  wsUrl: http://sensitive-data-service.vbp.internal:9189
  timeoutSec: 30
collectory:
  wsUrl: http://collectory.vbp.internal:8080/collectory/ws/
  timeoutSec: 30
  retryConfig:
    maxAttempts: 10
    initialIntervalMillis: 10000
  httpHeaders:
    Authorization: ${APIKEY}
    apiKey: ${APIKEY}
imageService:
  wsUrl: http://image-service.vbp.internal:8080/image-service/
  timeoutSec: 30
  httpHeaders:
    apiKey: ${APIKEY}
  retryConfig:
    maxAttempts: 5
    initialIntervalMillis: 1000
speciesListService:
  wsUrl: http://species-list.vbp.internal:8080/species-list/
  timeoutSec: 90
  retryConfig:
    maxAttempts: 5
    initialIntervalMillis: 1000
samplingService:
  wsUrl: http://spatial-service.vbp.internal:8080/spatial-service/
  timeoutSec: 60
  batchSize: 20000
  batchStatusSleepTime: 1000
  downloadRetries: 5
  retryConfig:
    maxAttempts: 10
    initialIntervalMillis: 1000
geocodeConfig:
  country:
    path: /data/pipelines-shp/political
    field: ISO_A2
    intersectBuffer: 0.135
    intersectMapping:
      CX: AU
      CC: AU
      HM: AU
      NF: AU
  eez:
    path: /data/pipelines-shp/eez
    field: ISO2
    intersectBuffer: 0.135
  stateProvince:
    path: /data/pipelines-shp/cw_state_poly
    field: FEATURE
    intersectBuffer: 0
  biome:
    path: /data/pipelines-shp/gadm0
    field: FEATURE

gbifConfig:
  extensionsAllowedForVerbatimSet:
    - http://rs.tdwg.org/ac/terms/Multimedia
    - http://data.ggbn.org/schemas/ggbn/terms/Amplification
    - http://data.ggbn.org/schemas/ggbn/terms/Cloning
    - http://data.ggbn.org/schemas/ggbn/terms/GelImage
    - http://data.ggbn.org/schemas/ggbn/terms/Loan
    - http://data.ggbn.org/schemas/ggbn/terms/MaterialSample
    - http://data.ggbn.org/schemas/ggbn/terms/Permit
    - http://data.ggbn.org/schemas/ggbn/terms/Preparation
    - http://data.ggbn.org/schemas/ggbn/terms/Preservation
    - http://rs.iobis.org/obis/terms/ExtendedMeasurementOrFact
    - http://rs.tdwg.org/chrono/terms/ChronometricAge
    - http://purl.org/germplasm/germplasmTerm#GermplasmAccession
    - http://purl.org/germplasm/germplasmTerm#MeasurementScore
    - http://purl.org/germplasm/germplasmTerm#MeasurementTrait
    - http://purl.org/germplasm/germplasmTerm#MeasurementTrial
    - http://rs.tdwg.org/dwc/terms/Identification
    - http://rs.tdwg.org/dwc/terms/Occurrence
    - http://rs.gbif.org/terms/1.0/Identifier
    - http://rs.gbif.org/terms/1.0/Image
    - http://rs.tdwg.org/dwc/terms/MeasurementOrFact
    - http://rs.gbif.org/terms/1.0/Multimedia
    - http://rs.gbif.org/terms/1.0/Reference
    - http://rs.tdwg.org/dwc/terms/ResourceRelationship
    - http://rs.gbif.org/terms/1.0/DNADerivedData
    - http://ala.org.au/terms/seedbank/0.1/SeedbankRecord
  vocabularyConfig:
    vocabulariesPath: /data/pipelines-vocabularies/
    vocabulariesNames:
      http://rs.tdwg.org/dwc/terms/degreeOfEstablishment: DegreeOfEstablishment
      http://rs.tdwg.org/dwc/terms/lifeStage: LifeStage
      http://rs.tdwg.org/dwc/terms/establishmentMeans: EstablishmentMeans
      http://rs.tdwg.org/dwc/terms/pathway: Pathway

alaNameMatchConfig:
  matchOnTaxonID: false

#locationInfoConfig:
#    countryNamesFile : /data/pipelines-data/resources/countries.txt
#    countryCentrePointsFile : /data/pipelines-data/resources/countryCentrePoints.txt
#    stateProvinceCentrePointsFile : /data/pipelines-data/resources/stateProvinceCentrePoints.txt
#    stateProvinceNamesFile : /data/pipelines-data/resources/stateProvinces.txt

### pipelines options: should match each GBIF PipelineOptions class options, that is, not extra options should be added
### on each yml category

# As PipelineOptions does not admits extra arguments, we use this comma separated list of --args to remove it after
# use it. In the case of fsPath be substitute '{fsPath}/pipelines-data' with fs.hdfs.fsPath if hdfs is selected in
# fs.platform or '/data' if 'local' is selected.
pipelineExcludeArgs: fsPath

# class: au.org.ala.pipelines.beam.ALADwcaToVerbatimPipeline
dwca-avro:
  runner: SparkRunner
  # Path of the input file
  inputPath: /data/biocache-load/{datasetId}/{datasetId}.zip
  targetPath: 's3://inbo-vbp-dev-pipelines/data/pipelines-data/{datasetId}/0/verbatim'
  tempLocation: /data/biocache-load/{datasetId}/tmp/

# class: au.org.ala.pipelines.beam.ALAVerbatimToInterpretedPipeline
interpret:
  appName: Interpretation for {datasetId}
  interpretationTypes: ALL
  defaultDateFormat: DMYT,DMY
  inputPath: 's3://inbo-vbp-dev-pipelines/data/pipelines-data/{datasetId}/0/verbatim/*.avro'
  targetPath: 's3://inbo-vbp-dev-pipelines/data/pipelines-data'
  # Skips GBIF id generation and copies ids from ExtendedRecord ids
  useExtendedRecordId: true
  runner: SparkRunner

# class: au.org.ala.pipelines.beam.ALAUUIDMintingPipeline
uuid:
  appName: UUID minting for {datasetId}
  runner: SparkRunner
  numberOfBackupsToKeep: 10
  percentageChangeAllowed: 50
  throwErrorOnValidationFail: false
  inputPath: 's3://inbo-vbp-dev-pipelines/data/pipelines-data'
  targetPath: 's3://inbo-vbp-dev-pipelines/data/pipelines-data'

images:
  appName: Image syncing for {datasetId}
  runner: SparkRunner
  tempLocation: /tmp
  # pipe separated list (note: YAML list not coming through correctly)
  recognisedPaths: "https://natuurdata.dev.inbo.be/images"
  inputPath: 's3://inbo-vbp-dev-pipelines/data/pipelines-data'
  targetPath: 's3://inbo-vbp-dev-pipelines/data/pipelines-data'

speciesLists:
  runner: SparkRunner
  speciesAggregatesPath: 's3://inbo-vbp-dev-pipelines/data/pipelines-species'
  tempLocation: /tmp
  maxDownloadAgeInMinutes: 1440
  includeConservationStatus: true
  includeInvasiveStatus: true

# Sampling specific configuration
sampling:
  appName: Sample for {datasetId}
  allDatasetsInputPath: 's3://inbo-vbp-dev-pipelines/data/pipelines-all-datasets'
  runner: SparkRunner
  inputPath: 's3://inbo-vbp-dev-pipelines/data/pipelines-data'

# Calculate distance to the expert distribution layers
# class:au.org.ala.pipelines.beam.DistributionOutlierPipeline
outlier:
  appName: Expert distribution outliers for {datasetId}
  baseUrl: https://spatial-service.vbp.internal:8080/spatial-service/
  targetPath: 'hdfs:///pipelines-outlier'
  allDatasetsInputPath: 'hdfs:///pipelines-all-datasets'
  runner: SparkRunner

annotation:
  runner: SparkRunner
  inputPath: s3://inbo-vbp-dev-pipelines/data/pipelines-annotations
  targetPath: s3://inbo-vbp-dev-pipelines/data/pipelines-annotations
  tempLocation: /tmp/

# class: au.org.ala.pipelines.beam.ALAInterpretedToSensitivePipeline
sensitive:
  appName: Sensitive Data Processing for {datasetId}
  runner: SparkRunner
  inputPath: 's3://inbo-vbp-dev-pipelines/data/pipelines-data'

# class: au.org.ala.pipelines.java.IndexRecordPipeline
index:
  appName: IndexRecord generation for {datasetId}
  includeImages: true
  includeSpeciesLists: true
  includeSensitiveDataChecks: false
  runner: SparkRunner
  allDatasetsInputPath: 's3://inbo-vbp-dev-pipelines/data/pipelines-all-datasets'
  inputPath: 's3://inbo-vbp-dev-pipelines/data/pipelines-data/'
  targetPath: s3://inbo-vbp-dev-pipelines/data/pipelines-data'


# class: au.org.ala.pipelines.beam.IndexRecordToSolrPipeline
solr:
  appName: SOLR indexing for {datasetId}
  allDatasetsInputPath: 's3://inbo-vbp-dev-pipelines/data/pipelines-all-datasets'
  jackKnifePath: "s3://inbo-vbp-dev-pipelines/pipelines-jackknife"
  clusteringPath: "s3://inbo-vbp-dev-pipelines/pipelines-clustering"
  outlierPath: 's3://inbo-vbp-dev-pipelines/pipelines-outlier'
  annotationsPath: 's3://inbo-vbp-dev-pipelines/pipelines-annotations'
  solrCollection: biocache
  includeSampling: true
  includeJackKnife: false
  includeClustering: false
  runner: SparkRunner
  zkHost: solr.vbp.internal:9983
  numOfPartitions: 12

# class: au.org.ala.pipelines.beam.IndexRecordToSolrPipeline
elastic:
  appName: Elastic indexing for {datasetId}
  runner: SparkRunner
  esHosts: http://elasticsearch.vbp.internal:9200
  esAlias: event
  esIndexName: 'event_{datasetId}'
  indexNumberShards: 12
  indexNumberReplicas: 0
  metaFileName: elastic.yml
  esDocumentId: internalId
  inputPath: 's3://inbo-vbp-dev-pipelines/data/'


export:
  appName: DwCA export for {datasetId}
  inputPath: '{fsPath}/pipelines-data'
  targetPath: '{fsPath}/pipelines-data'
  localExportPath: /tmp/pipelines-export
  allDatasetsInputPath: '{fsPath}/pipelines-all-datasets'
  imageServicePath: "https://images.biodiversiteitsportaal/image/proxyImageThumbnailLarge?imageId={0}"
  runner: SparkRunner
  predicateExportEnabled: true

predicate-export:
  inputPath: '{fsPath}/pipelines-data'
  targetPath: '{fsPath}/pipelines-export'
  localExportPath: /tmp/pipelines-export
  skipEventExportFields:
    - "id"
    - "parentId"
    - "created"
    - "hasGeospatialIssue"
    - "hasCoordinate"
    - "repatriated"
    - "coreId"
  skipOccurrenceExportFields:
    - "id"
    - "parentId"
    - "created"
    - "hasGeospatialIssue"
    - "hasCoordinate"
    - "repatriated"
    - "coreId"

# class: au.org.ala.pipelines.beam.IndexRecordToSolrPipeline
solrSchema:
  appName: SOLR Schema Sync

# JackKnife specific configuration
# class: au.org.ala.pipelines.beam.ALAReverseJackKnifePipeline
jackKnife:
  appName: Jack knife outlier detection
  jackKnifePath: "{fsPath}/pipelines-jackknife"
  layers: cl10001,cl10000,cl10002
  minSampleThreshold: 80
  allDatasetsInputPath: '{fsPath}/pipelines-all-datasets'
  runner: SparkRunner

clustering:
  appName: Occurrence clustering
  clusteringPath: "{fsPath}/pipelines-clustering"
  allDatasetsInputPath: '{fsPath}/pipelines-all-datasets'
  runner: SparkRunner

# class: DumpArchiveList
dataset-archive-list:
  inputPath: '{fsPath}/dwca-imports'
  targetPath: /tmp/dataset-archive-list.csv

# class: au.org.ala.utils.DumpDatasetSize
dataset-count-dump:
  inputPath: '{fsPath}/pipelines-data'
  targetPath: /tmp/dataset-counts.csv
  dwcaImportPath: '{fsPath}/dwca-imports'

validation-report:
  inputPath: '{fsPath}/pipelines-data'
  targetPath: /tmp/dataset-validation-list.csv
  fullReportPath: /tmp/full-validation-list.csv
  zkHost: solr.vbp.internal:9983
  checkSolr: true
  checkSampling: true
  solrCollection: biocache



-dwca-avro-sh-args:
  local:
    jvm: -Xmx29g
  spark-embedded:
    jvm: -Xmx29g
  spark-cluster:
    conf: spark.default.parallelism=144
    num-executors: 6
    executor-cores: 8
    executor-memory: 24G
    driver-memory: 4G

### la-pipelines cli additional arguments, like JVM or spark command line arguments
interpret-sh-args:
  local:
    jvm: -Xmx29g -XX:+UseG1GC -Dspark.master=local[*]
  spark-embedded:
    jvm: -Xmx29g -XX:+UseG1GC -Dspark.master=local[*]
  spark-cluster:
    conf: spark.default.parallelism=144
    num-executors: 14
    executor-cores: 16
    executor-memory: 29G
    driver-memory: 4G

image-sync-sh-args:
  local:
    jvm: -Xmx29g -XX:+UseG1GC -Dspark.master=local[*]
  spark-embedded:
    jvm: -Xmx29g -XX:+UseG1GC -Dspark.master=local[*]
  spark-cluster:
    conf: spark.default.parallelism=144
    num-executors: 16
    executor-cores: 8
    executor-memory: 7G
    driver-memory: 1G

image-load-sh-args:
  local:
    jvm: -Xmx29g -XX:+UseG1GC -Dspark.master=local[*]
  spark-embedded:
    jvm: -Xmx29g -XX:+UseG1GC -Dspark.master=local[*]
  spark-cluster:
    conf: spark.default.parallelism=144
    num-executors: 16
    executor-cores: 8
    executor-memory: 7G
    driver-memory: 1G

annotations-sh-args:
  local:
    jvm: -Xmx29g -XX:+UseG1GC -Dspark.master=local[*]
  spark-embedded:
    jvm: -Xmx29g -XX:+UseG1GC -Dspark.master=local[*]
  spark-cluster:
    conf: spark.default.parallelism=144
    num-executors: 16
    executor-cores: 8
    executor-memory: 7G
    driver-memory: 1G

export-sh-args:
  local:
    jvm: -Xmx29g -XX:+UseG1GC -Dspark.master=local[*]
  spark-embedded:
    jvm: -Xmx29g -XX:+UseG1GC -Dspark.master=local[*]
  spark-cluster:
    conf: spark.default.parallelism=144
    num-executors: 16
    executor-cores: 8
    executor-memory: 7G
    driver-memory: 1G

uuid-sh-args:
  local:
    jvm: -Xmx29g -XX:+UseG1GC
  spark-embedded:
    jvm: -Xmx29g -XX:+UseG1GC
  spark-cluster:
    conf: spark.default.parallelism=500
    num-executors: 14
    executor-cores: 16
    executor-memory: 29G
    driver-memory: 4G

sampling-sh-args:
  local:
    jvm: -Xmx29g -XX:+UseG1GC -Dspark.master=local[*]
  spark-embedded:
    jvm: -Xmx29g -XX:+UseG1GC -Dspark.master=local[*]
  spark-cluster:
    conf: spark.default.parallelism=144
    num-executors: 8
    executor-cores: 8
    executor-memory: 29g
    driver-memory: 4G

outlier-sh-args:
  local:
    jvm: -Xmx29g -XX:+UseG1GC -Dspark.master=local[*]
  spark-embedded:
    jvm: -Xmx29g -XX:+UseG1GC -Dspark.master=local[*]
  spark-cluster:
    conf: spark.default.parallelism=144
    num-executors: 8
    executor-cores: 8
    executor-memory: 29g
    driver-memory: 4G

sensitive-sh-args:
  spark-embedded:
    jvm: -Xmx29g -XX:+UseG1GC
  spark-cluster:
    jvm: -Xmx29g -XX:+UseG1GC
    num-executors: 8
    executor-cores: 8
    executor-memory: 29G
    driver-memory: 4G

sample-sh-args:
  local:
    jvm: -Xmx29g -XX:+UseG1GC

index-sh-args:
  local:
    jvm: -Xmx29g -XX:+UseG1GC
  spark-embedded:
    jvm: -Xmx29g -XX:+UseG1GC
  spark-cluster:
    conf: spark.default.parallelism=48
    num-executors: 14
    executor-cores: 16
    executor-memory: 29G
    driver-memory: 4G

jackknife-sh-args:
  local:
    jvm: -Xmx29g -XX:+UseG1GC
  spark-embedded:
    jvm: -Xmx29g -XX:+UseG1GC
  spark-cluster:
    conf: spark.default.parallelism=500
    num-executors: 24
    executor-cores: 8
    executor-memory: 7G
    driver-memory: 1G

clustering-sh-args:
  local:
    jvm: -Xmx29g -XX:+UseG1GC
  spark-embedded:
    jvm: -Xmx29g -XX:+UseG1GC
  spark-cluster:
    conf: spark.default.parallelism=500
    num-executors: 6
    executor-cores: 8
    executor-memory: 20G
    driver-memory: 4G

solr-sh-args:
  local:
    jvm: -Xmx29g -XX:+UseG1GC
  spark-embedded:
    jvm: -Xmx29g -XX:+UseG1GC
  spark-cluster:
    conf: spark.default.parallelism=500
    num-executors: 14
    executor-cores: 16
    executor-memory: 29G
    driver-memory: 6G

elastic-sh-args:
  local:
    jvm: -Xmx29g -XX:+UseG1GC
  spark-embedded:
    jvm: -Xmx29g -XX:+UseG1GC
  spark-cluster:
    conf: spark.default.parallelism=48
    num-executors: 6
    executor-cores: 8
    executor-memory: 22G
    driver-memory: 6G

